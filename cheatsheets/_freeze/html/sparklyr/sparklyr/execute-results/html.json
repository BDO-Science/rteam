{
  "hash": "319f3fddc68fd7b995e659573b518077",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Data Science in Spark with Sparklyr :: Cheat Sheet\"\nformat: \n  html:\n    toc: true\n    highlight-style: a11y-dark\neditor: visual\n---\n\n\n\n\n\n## Intro\n\nsparklyr is an R interface for **Apache Spark**, it provides a complete **dplyr** backend and the option to query directly using **Spark SQL** statement. With **sparklyr**, you can orchestrate distributed machine learning using either **Spark's MLlib** or **H2O** Sparkling Water. Starting with **version 1.044**, **RStudio Desktop**, **Server and Pro include integrated support for the sparklyr package**. You can create and manage connections to Spark clusters and local Spark instances from inside the IDE.\n\n### RStudio Integrates with sparklyr\n\nTODO Screenshots\n\nExpand to read about the sparklyr features in the RStudio IDE.\n\n#### Sparklyr features in the RStudio IDE\n\n-   Open connection log\n-   Disconnect\n-   Open the Spark UI\n-   Spark & Hive Tables\n-   Preview 1K rows\n\n## Cluster Deployment\n\nIn a managed cluster, the driver node (RStudio, Spark, Hive) connects to the cluster manager (Yarn, Mesos) which connects to the worker nodes (Spark).\n\nIn a stand alone cluster the driver node (RStudio, Spark) connects directly to the worker nodes (Spark).\n\n## Data Science Toolchain with Spark + sparklyr\n\n1.  Import\n    -   Export an R DataFrame\n\n    -   Read a file\n\n    -   Read existing Hive table\n2.  Tidy/Wrangle\n    -   dplyr verb\n\n    -   Direct Spark SQL (DBI)\n\n    -   SDF function (Scala API)\n3.  Understand\n    -   Transform - Transformer function\n\n    -   Visualize - Collect data into R for plotting\n\n    -   Model - Spark MLlib and H2O Extension\n4.  Communicate\n    -   Collect data into R\n\n    -   Share plots, documents, and apps\n\n## Getting Started\n\n### Local Mode (no cluster required)\n\n1.  Install a local version of Spark:\n\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    spark_install(\"2.0.1\")\n    ```\n    :::\n\n\n\n\n\n2.  Open a connection:\n\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    sc <- spark_connect(master = \"local\")\n    ```\n    :::\n\n\n\n\n\n### On a Mesos Managed Cluster\n\n1.  Install RStudio Server or Pro on one of the existing nodes\n\n2.  Locate path to the cluster's Spark directory\n\n3.  Open a connection\n\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    spark_connect(master = \"[mesos URL]\",\n                  version = \"1.6.2\", \n                  spark_home = [Cluster’s Spark path])\n    ```\n    :::\n\n\n\n\n\n### Using Livy (Experimental)\n\n1.  The Livy REST application should be running on the cluster\n\n2.  Connect to the cluster\n\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    sc <- spark_connect(method = \"livy\", \n                        master = \"http://host:port\")\n    ```\n    :::\n\n\n\n\n\n### On a Yarn Managed Cluster\n\n1.  Install RStudio Server or RStudio Pro on one of the existing nodes, preferably an edge node\n\n2.  Locate path to the cluster's Spark Home Directory, it normally is `/usr/lib/spark`\n\n3.  Open a connection\n\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    spark_connect(master=\"yarn-client\", \n                  version = \"1.6.2\", \n                  spark_home = [Cluster’s Spark path])\n    ```\n    :::\n\n\n\n\n\n### On a Spark Standaline Cluster\n\n1.  Install RStudio Server or RStudio Pro on one of the existing nodes or a server in the same LAN\n\n2.  Install a local version of Spark:\n\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    spark_install(version = \"2.0.1\")\n    ```\n    :::\n\n\n\n\n\n3.  Open a connection\n\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    spark_connect(master=\"spark:// host:port\",\n                  version = \"2.0.1\", \n                  spark_home = spark_home_dir())\n    ```\n    :::\n\n\n\n\n\n## Tuning Spark\n\n### Example Configuration\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfig <- spark_config() \nconfig$spark.executor.cores <- 2\nconfig$spark.executor.memory <- \"4G\" \nsc <- spark_connect (master=\"yarn-client\", config = config, version = \"2.0.1\")\n```\n:::\n\n\n\n\n\n### Important Tuning Parameters (with defaults)\n\n-   `spark.yarn.am.cores`\n-   `spark.yarn.am.memory`: 512m\n-   `spark.network.timeout`: 120s\n-   `spark.executor.memory`: 1g\n-   `spark.executor.cores`: 1\n-   `spark.executor.instances`\n-   `spark.executor.extraJavaOptions`\n-   `spark.executor.heartbeatInterval`: 10s\n-   `sparklyr.shell.executor-memory`\n-   `sparklyr.shell.driver-memory`\n\n## Using sparklyr\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nset.seed(100)\n\n#Install Spark locally\nspark_install(\"2.0.1\")\n\n# Connect to local version\nsc <- spark_connect(master = \"local\")\n\n# Copy data to Spark memory\nimport_iris <- copy_to(sc, \n                       iris, \n                       \"spark_iris\", \n                       overwrite = TRUE)\n\n# Partition data\npartition_iris <- sdf_partition(import_iris,\n                                training = 0.5, \n                                testing = 0.5)\n\n#Create a hive metadata for each partition\n\nsdf_register(partition_iris,\n             c(\"spark_iris_training\", \"spark_iris_test\"))\n      \nspark_connect(master = \"[mesos URL]\", \n              version = \"1.6.2\", spark_home = [Cluster’s Spark path])\n\ntidy_iris <- tbl(sc, \"spark_iris_training\") %>% \n  select(Species, Petal_Length, Petal_Width)\n\n# Spark ML Decision Tree Model\nmodel_iris <- tidy_iris %>%\n  ml_decision_tree(response = \"Species\",\n                   features = c(\"Petal_Length\", \"Petal_Width\"))\n\n# Create reference to Spark table\ntest_iris <- tbl(sc, \"spark_iris_test\")\n\n# Bring data back into R memory for plotting\npred_iris <- sdf_predict(model_iris, test_iris) %>% \n  collect\n\npred_iris %>% inner_join(data.frame(prediction = 0:2, lab = model_iris$model.parameters$labels)) %>%\n  ggplot(aes(Petal_Length, Petal_Width, col = lab)) + geom_point()\n\n# Disconnect\nspark_disconnect(sc)\n```\n:::\n\n\n\n\n\n<!-- page 2 -->\n\n## Reactivity\n\n### Copy a Data Frame Into Spark\n\n-   `sdf_copy_to(sc, x, name, memory, repartition, overwrite)`\n\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    sdf_copy_to(sc, iris, \"spark_iris\")\n    ```\n    :::\n\n\n\n\n\n### Import Into Spark From a File\n\nArguments that apply to all functions: `sc`, `name`, `path`, `options = list()`, `repartition = 0`, `memory = TRUE`, `overwrite = TRUE`\n\n-   `spark_read_csv(header = TRUE, columns = NULL, infer_schema = TRUE, delimiter = \",\", quote = \"\\\"\", escape = \"\\\\\", charset = \"UTF-8\", null_value = NULL)`\n\n-   `spark_read_json()`\n\n-   `spark_read_parquet()`\n\n### Spark SQL Commands\n\n-   `DBI::dbWriteTable(conn, value)`\n\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    DBI::dbWriteTable(sc, \"spark_iris\", iris)\n    ```\n    :::\n\n\n\n\n\n### From a Table in Hive\n\n-   `tbl_cache(sc, name, force = TRUE)`: Loads the table into memory\n\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    my_var <- tbl_cache(sc, name= \"hive_iris\")\n    ```\n    :::\n\n\n\n\n\n-   `dplyr::tbl(scr, ...)`: Creates a reference to the table without loading it into memory\n\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    my_var <- dplyr::tbl(sc, name= \"hive_iris\")\n    ```\n    :::\n\n\n\n\n\n## Wrangle\n\n### Spark SQL via dplyer Verbs\n\n-   Translates into Spark SQL statements:\n\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    my_table <- my_var %>% \n      filter(Species==\"setosa\") %>% \n      sample_n(10)\n    ```\n    :::\n\n\n\n\n\n### Direct Spark SQL Commands\n\n-   `DBI::dbGetQuery(conn, statement)`\n\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    my_table <- DBI::dbGetQuery(sc, \"SELECT * FROM iris LIMIT 10\")\n    ```\n    :::\n\n\n\n\n\n### Scala API via SDF Functions\n\n-   `sdf_mutate(.data)`: Works like dplyr mutate function\n\n-   `sdf_partition(x, ..., weights = NULL, seed = sample (.Machine$integer.max, 1))`\n\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    sdf_partition(x, training = 0.5, test = 0.5) sdf_register(x, name = NULL)\n    ```\n    :::\n\n\n\n\n\n-   `sdf_register(x, name = NULL)`: Gives a Spark DataFrame a table name\n\n-   `sdf_sample(x, fraction = 1, replacement = TRUE, seed = NULL)`\n\n-   `sdf_sort(x, columns)`: Sorts by \\>=1 columns in ascending order\n\n-   `sdf_with_unique_id(x, id = \"id\")`\n\n-   `sdf_predict(object, newdata)`: Spark DataFrame with predicted values\n\n### ML Transformers\n\nExample:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nft_binarizer(my_table,\n             input.col=\"Petal_Length\", \n             output.col=\"petal_large\", \n             threshold=1.2)\n```\n:::\n\n\n\n\n\nArguments that apply to all functions: `x`, `input.col = NULL`, `output.col = NULL`\n\n-   `ft_binarizer(threshold = 0.5)`: Assigned values based on threshold\n\n-   `ft_bucketizer(splits)`: Numeric column to discretized column\n\n-   `ft_discrete_cosine_transform(inverse = FALSE)`: Time domain to frequency domain\n\n-   `ft_elementwise_product(scaling.col)`: Element-wise product between 2 cols\n\n-   `ft_index_to_string()`: Index labels back to label as strings\n\n-   `ft_one_hot_encoder()`: Continuous to binary vectors\n\n-   `ft_quantile_discretizer(n.buckets=5L)`: Continuous to binned categorical values\n\n-   `ft_sql_transformer(sql)`\n\n-   `ft_string_indexer(params = NULL)`: Column of labels into a column of label indices\n\n-   `ft_vector_assembler()`: Combine vectors into single row-vector\n\n## Visulize & Communicate\n\n### Download Data to R Memory\n\nExample:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr_table <- collect(my_table) \nplot(Petal_Width~Petal_Length, \n     data=r_table)\n```\n:::\n\n\n\n\n\n-   `dplyr::collect(x)`: Download a Spark DataFrame to an R DataFrame\n\n-   `sdf_read_column(x, column)`: Returns contents of a single column to R\n\n### Save From Spark to File System\n\nArguments that apply to all functions: `x`, `path`\n\n-   `spark_read_csv( header = TRUE, delimiter = \",\", quote = \"\\\"\", escape = \"\\\\\", charset = \"UTF-8\", null_value = NULL)`\n\n-   `spark_read_json(mode = NULL)`\n\n-   `spark_read_parquet(mode = NULL)`\n\n## Reading & Writing from Apache Spark\n\nWrite to Spark, from R with `sdf_copy_to()`, `dplyr::copy_to()`, or `DBI::sbWriteTable()`.\n\nRead from Spark, to R with `sdf_collect()`, `dplyr::collect()`, `sdf_read_column`.\n\n------------------------------------------------------------------------\n\nWrite to Spark, from Hive with `tbl_cache()` or `dplyr::tbl()`.\n\n------------------------------------------------------------------------\n\nWrite to Spark from the file system with `spark_read_<fmt>()`.\n\nRead from Spark to the file system with `spark_write_<fmt>()`.\n\n## Extensions\n\nCreate an R package that calls the full Spark API & provide interfaces to Park packages.\n\n### Core Types\n\n-   `spark_connection()`: Connection between R and the Spark shell process\n\n-   `spark_jobj()`: Instance og a remote Spark object\n\n-   `spark_dataframe()`: Instance of a remote Spark DataFrame object\n\n### Call Spark From R\n\n-   `invoke()`: Call a method on a Java object\n\n-   `invoke_new()`: Create a new object by invoking a constructor\n\n-   `invoke_static()`: Call a static method on an object\n\n### Machine Learning Extensions\n\n-   `ml_create_dummy_variables()`\n\n-   `ml_prepare_dataframe()`\n\n-   `ml_prepare_response_features_intercept()`\n\n-   `ml_options()`\n\n-   `ml_model()`\n\n## Model (MLlib)\n\nExample:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nml_decision_tree(my_table, \n                 response = \"Species\", features = c(\"Petal_Length\" , \"Petal_Width\"))\n```\n:::\n\n\n\n\n\n-   `ml_als_factorization(x, user.column = \"user\", rating.column = \"rating\", item.column = \"item\", rank = 10L, regularization.parameter = 0.1, iter.max = 10L, ml.options = ml_options())`\n\n-   `ml_decision_tree(x, response, features, max.bins = 32L, max.depth = 5L, type = c(\"auto\", \"regression\", \"classification\"), ml.options = ml_options())`: Same options for: ml_gradient_boosted_trees\n\n-   `ml_generalized_linear_regression(x, response, features, intercept = TRUE, family = gaussian(link = \"identity\"), iter.max = 100L, ml.options = ml_options())`\n\n-   `ml_kmeans(x, centers, iter.max = 100, features = dplyr::tbl_vars(x), compute.cost = TRUE, tolerance = 1e-04, ml.options = ml_options())`\n\n-   `ml_lda(x, features = dplyr::tbl_vars(x), k = length(features), alpha = (50/k) + 1, beta = 0.1 + 1, ml.options = ml_options())`\n\n-   `ml_linear_regression(x, response, features, intercept = TRUE, alpha = 0, lambda = 0, iter.max = 100L, ml.options = ml_options())`: Same options for: ml_logistic_regression\n\n-   `ml_multilayer_perceptron(x, response, features, layers, iter.max = 100, seed = sample(.Machine$integer.max, 1), ml.options = ml_options())`\n\n-   `ml_naive_bayes(x, response, features, lambda = 0, ml.options = ml_options())`\n\n-   `ml_one_vs_rest(x, classifier, response, features, ml.options = ml_options())`\n\n-   `ml_pca(x, features = dplyr::tbl_vars(x), ml.options = ml_options())`\n\n-   `ml_random_forest(x, response, features, max.bins = 32L, max.depth = 5L, num.trees = 20L, type = c(\"auto\", \"regression\", \"classification\"), ml.options = ml_options())`\n\n-   `ml_survival_regression(x, response, features, intercept = TRUE,censor = \"censor\", iter.max = 100L, ml.options = ml_options())`\n\n-   `ml_binary_classification_eval(predicted_tbl_spark, label, score, metric = \"areaUnderROC\")`\n\n-   `ml_classification_eval(predicted_tbl_spark, label, predicted_lbl, metric = \"f1\")`\n\n-   `ml_tree_feature_importance(sc, model)`\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}